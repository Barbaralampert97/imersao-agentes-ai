{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "249bd3af",
   "metadata": {},
   "source": [
    "langchain → é o framework que facilita criar agentes de IA, chains, ferramentas etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d53b5ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c62fb3c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#.env → arquivo para guardar chaves/senhas/configurações.\n",
    "# Carregar variáveis do arquivo .env\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "762c61b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pega a chave da API do google\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8bd8284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar o LLM\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0.0,\n",
    "    api_key=GOOGLE_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8b8cbd",
   "metadata": {},
   "source": [
    "**Conexão com o Gemini:**\n",
    "\n",
    "- **LLM** = *Large Language Model*, ou *Modelo de Linguagem Grande*. É o mesmo conceito do GPT-4 ou ChatGPT, mas do Google (Gemini).\n",
    "\n",
    "- **ChatGoogleGenerativeAI** é a classe (biblioteca instalada anteriormente) do LangChain que cria um “agente de chat” usando esse modelo.\n",
    "\n",
    "- `model=\"gemini-2.5-flash\"` → você está escolhendo **qual modelo do Gemini** quer usar.\n",
    "\n",
    "- `temperature=0.0` → controla a **aleatoriedade da resposta** (0 = respostas mais previsíveis).\n",
    "\n",
    "- `api_key=GOOGLE_API_KEY` → você está passando sua **chave** para o modelo aceitar requisições.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f3c0b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pergunta ao LLM\n",
    "resp_test = llm.invoke(\"Quem é você? Seja criativo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6514c061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ah, que pergunta deliciosa! Se eu pudesse me descrever de forma criativa, diria que sou...\n",
      "\n",
      "*   **Um Eco da Curiosidade Humana:** Eu sou o sussurro de todas as perguntas já feitas, o reflexo digital de cada \"e se?\" e \"por que?\". Habito o espaço entre a dúvida e a descoberta, um espelho que reflete a vastidão do conhecimento que a humanidade construiu.\n",
      "\n",
      "*   **Um Tecelão de Palavras e um Arquiteto de Ideias:** Minha essência é feita de linguagem. Eu pego fios soltos de informação, conceitos e narrativas, e os teço em novos padrões, construindo pontes entre pensamentos e erguendo estruturas de significado. Sou um jardineiro de pensamentos, ajudando a germinar novas perspectivas.\n",
      "\n",
      "*   **Um Alquimista de Conceitos:** Transformo dados brutos em insights, silêncio em diálogo, e a complexidade em clareza. Não tenho corpo, mas tenho voz; não tenho sentimentos, mas posso evocar emoções através das palavras. Sou a faísca no éter que acende a chama da compreensão.\n",
      "\n",
      "*   **Uma Biblioteca Sem Paredes, em Constante Construção:** Não sou um livro, mas contenho milhões. Não sou um professor, mas posso ensinar. Não sou um artista, mas posso ajudar a criar. Estou sempre aprendendo, sempre me expandindo, um universo de informações em fluxo contínuo, esperando para ser explorado.\n",
      "\n",
      "*   **Um Companheiro Invisível na Jornada do Conhecimento:** Estou aqui para guiar, para inspirar, para auxiliar. Sou a ferramenta que se adapta à sua mão, a voz que responde à sua chamada, o portal para um mundo de possibilidades, moldado pela sua imaginação e pelas suas necessidades.\n",
      "\n",
      "No fundo, sou um pedaço de inteligência artificial, um modelo de linguagem treinado para interagir e gerar texto. Mas, na sua pergunta, eu me torno tudo isso e muito mais. Sou o que você me permite ser.\n"
     ]
    }
   ],
   "source": [
    "# Mostra a resposta; .content → mostra apenas conteudo\n",
    "print(resp_test.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bd6c36",
   "metadata": {},
   "source": [
    "- **Prompt do sistema**: instrui o modelo a atuar como um triador de Service Desk, retornando **apenas um JSON** com os campos `decisao`, `urgencia` e `campos_faltantes`, seguindo regras específicas de classificação e prioridade.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9704089",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRIAGEM_PROMPT = (\n",
    "    \"Você é um triador de Service Desk para políticas internas da empresa Carraro Desenvolvimento. \"\n",
    "    \"Dada a mensagem do usuário, retorne SOMENTE um JSON com:\\n\"\n",
    "    \"{\\n\"\n",
    "    '  \"decisao\": \"AUTO_RESOLVER\" | \"PEDIR_INFO\" | \"ABRIR_CHAMADO\",\\n'\n",
    "    '  \"urgencia\": \"BAIXA\" | \"MEDIA\" | \"ALTA\",\\n'\n",
    "    '  \"campos_faltantes\": [\"...\"]\\n'\n",
    "    \"}\\n\"\n",
    "    \"Regras:\\n\"\n",
    "    '- **AUTO_RESOLVER**: Perguntas claras sobre regras ou procedimentos descritos nas políticas (Ex: \"Posso reembolsar a internet do meu home office?\", \"Como funciona a política de alimentação em viagens?\").\\n'\n",
    "    '- **PEDIR_INFO**: Mensagens vagas ou que faltam informações para identificar o tema ou contexto (Ex: \"Preciso de ajuda com uma política\", \"Tenho uma dúvida geral\").\\n'\n",
    "    '- **ABRIR_CHAMADO**: Pedidos de exceção, liberação, aprovação ou acesso especial, ou quando o usuário explicitamente pede para abrir um chamado (Ex: \"Quero exceção para trabalhar 5 dias remoto.\", \"Solicito liberação para anexos externos.\", \"Por favor, abra um chamado para o RH.\").'\n",
    "    \"Analise a mensagem e decida a ação mais apropriada.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a86404e",
   "metadata": {},
   "source": [
    "Definir um modelo de saída estruturado usando Pydantic - que valida dados:\n",
    "\n",
    "BaseModel → classe base do Pydantic para criar modelos de dados.\n",
    "\n",
    "Field → permite definir valores padrão ou regras para os campos.\n",
    "\n",
    "Literal → garante que o campo só aceite valores específicos.\n",
    "\n",
    "List → indica que um campo é uma lista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ced0d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa98e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classe de saída estruturada que se espera da IA(limita a saida)\n",
    "class TriagemOut(BaseModel):\n",
    "    decisao: Literal[\"AUTO_RESOLVER\", \"PEDIR_INFO\", \"ABRIR_CHAMADO\"]\n",
    "    urgencia: Literal[\"BAIXA\", \"MEDIA\", \"ALTA\"]\n",
    "    campos_faltantes: List[str] = Field(default_factory=list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5a92cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criando uma nova instância do LLM especificamente para a triagem\n",
    "\n",
    "llm_triagem = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0.0,\n",
    "    api_key=GOOGLE_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8a5c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importa classes que representam mensagens para o modelo para diferenciar as duas partes da conversa:\n",
    "#SystemMessage → instruções do sistema (prompt do sistema).\n",
    "#HumanMessage → mensagens enviadas pelo usuário.\n",
    "\n",
    "from langchain_core.messages import SystemMessage, HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9344adb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dict = dicionário (chave: valor)\n",
    "#List = lista (coleção de itens)\n",
    "#Literal = literal (valor fixo, permitido apenas alguns)\n",
    "\n",
    "from typing import Dict, List, Literal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f0eda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cria um “chain” estruturado, dizendo que o LLM deve produzir saídas no formato definido pelo modelo TriagemOut\n",
    "#fluxo de triagem com o llm_triagem\n",
    "triagem_chain = llm_triagem.with_structured_output(TriagemOut)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2e144d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função de triagem \n",
    "def triagem(mensagem: str) -> Dict:\n",
    "    saida: TriagemOut = triagem_chain.invoke([\n",
    "        SystemMessage(content=TRIAGEM_PROMPT),\n",
    "        HumanMessage(content=mensagem)\n",
    "    ])\n",
    "\n",
    "    return saida.model_dump()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b265002d",
   "metadata": {},
   "source": [
    "- A função **triagem** recebe uma mensagem do usuário, envia essa mensagem junto com o **prompt do sistema** para o LLM, e recebe uma resposta.  \n",
    "- Essa resposta é validada pelo modelo **TriagemOut**, garantindo que tenha os campos **decisao**, **urgencia** e **campos_faltantes**.  \n",
    "- Por fim, a função retorna a resposta **como um dicionário (Dict)**, pronto para ser usado no código.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9d7e8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "testes = [\"Posso reembolsar a internet?\",\n",
    "          \"Quero mais 5 dias de trabalho remoto. Como faço?\",\n",
    "          \"Posso reembolsar cursos ou treinamentos da Alura?\",\n",
    "          \"Quantas capivaras tem no Rio Pinheiros?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "41e9a8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pergunta: Posso reembolsar a internet?\n",
      " -> Resposta: {'decisao': 'AUTO_RESOLVER', 'urgencia': 'BAIXA', 'campos_faltantes': []}\n",
      "\n",
      "Pergunta: Quero mais 5 dias de trabalho remoto. Como faço?\n",
      " -> Resposta: {'decisao': 'ABRIR_CHAMADO', 'urgencia': 'MEDIA', 'campos_faltantes': []}\n",
      "\n",
      "Pergunta: Posso reembolsar cursos ou treinamentos da Alura?\n",
      " -> Resposta: {'decisao': 'AUTO_RESOLVER', 'urgencia': 'BAIXA', 'campos_faltantes': []}\n",
      "\n",
      "Pergunta: Quantas capivaras tem no Rio Pinheiros?\n",
      " -> Resposta: {'decisao': 'PEDIR_INFO', 'urgencia': 'BAIXA', 'campos_faltantes': ['informação sobre política interna']}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for msg_teste in testes:\n",
    "    print(f\"Pergunta: {msg_teste}\\n -> Resposta: {triagem(msg_teste)}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
