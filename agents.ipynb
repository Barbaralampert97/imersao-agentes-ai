{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e894e369",
   "metadata": {},
   "source": [
    "1. Configuração Inicial e LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53b5ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa bibliotecas necessárias para variáveis de ambiente e o modelo Gemini\n",
    "import os \n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62fb3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#.env → arquivo para guardar chaves/senhas/configurações.\n",
    "# Carrega variáveis de ambiente do arquivo .env\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762c61b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtém a chave da API do Google das variáveis de ambiente\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bd8284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializa o modelo de linguagem grande (LLM) do Google Gemini\n",
    "# model: Define o modelo Gemini a ser utilizado (ex: \"gemini-2.5-flash\")\n",
    "# temperature: Controla a aleatoriedade da resposta (0.0 para respostas mais previsíveis)\n",
    "# api_key: Chave de autenticação para a API do Google\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0.0,\n",
    "    api_key=GOOGLE_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3c0b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de invocação do LLM para teste\n",
    "resp_test = llm.invoke(\"Quem é você? Seja criativo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6514c061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Imprime o conteúdo da resposta do LLM; \n",
    "# .content → mostra apenas conteudo\n",
    "print(resp_test.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cab3ae8",
   "metadata": {},
   "source": [
    "2. Prompt de Triagem e Saída Estruturada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9704089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define o prompt do sistema para o triador de Service Desk\n",
    "# O prompt instrui o modelo a retornar um JSON com decisão, urgência e campos faltantes\n",
    "TRIAGEM_PROMPT = (\n",
    "    \"Você é um triador de Service Desk para políticas internas da empresa Carraro Desenvolvimento. \"\n",
    "    \"Dada a mensagem do usuário, retorne SOMENTE um JSON com:\\n\"\n",
    "    \"{\\n\"\n",
    "    '  \"decisao\": \"AUTO_RESOLVER\" | \"PEDIR_INFO\" | \"ABRIR_CHAMADO\",\\n'\n",
    "    '  \"urgencia\": \"BAIXA\" | \"MEDIA\" | \"ALTA\",\\n'\n",
    "    '  \"campos_faltantes\": [\"...\"]\\n'\n",
    "    \"}\\n\"\n",
    "    \"Regras:\\n\"\n",
    "    '- **AUTO_RESOLVER**: Perguntas claras sobre regras ou procedimentos descritos nas políticas (Ex: \"Posso reembolsar a internet do meu home office?\", \"Como funciona a política de alimentação em viagens?\").\\n'\n",
    "    '- **PEDIR_INFO**: Mensagens vagas ou que faltam informações para identificar o tema ou contexto (Ex: \"Preciso de ajuda com uma política\", \"Tenho uma dúvida geral\").\\n'\n",
    "    '- **ABRIR_CHAMADO**: Pedidos de exceção, liberação, aprovação ou acesso especial, ou quando o usuário explicitamente pede para abrir um chamado (Ex: \"Quero exceção para trabalhar 5 dias remoto.\", \"Solicito liberação para anexos externos.\", \"Por favor, abra um chamado para o RH.\").'\n",
    "    \"Analise a mensagem e decida a ação mais apropriada.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ced0d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa classes do Pydantic para definir um modelo de dados estruturado\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa98e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a classe de saída esperada da IA, garantindo a estrutura do JSON\n",
    "# BaseModel: Classe base para modelos de dados Pydantic\n",
    "# Field: Permite definir valores padrão ou regras para os campos\n",
    "# Literal: Garante que o campo aceite apenas valores específicos\n",
    "# List: Indica que um campo é uma lista\n",
    "class TriagemOut(BaseModel):\n",
    "    decisao: Literal[\"AUTO_RESOLVER\", \"PEDIR_INFO\", \"ABRIR_CHAMADO\"]\n",
    "    urgencia: Literal[\"BAIXA\", \"MEDIA\", \"ALTA\"]\n",
    "    campos_faltantes: List[str] = Field(default_factory=list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a92cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criando uma nova instância do LLM  para a função triagem\n",
    "\n",
    "llm_triagem = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0.0,\n",
    "    api_key=GOOGLE_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8a5c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importa classes para diferenciar mensagens do sistema e do usuário\n",
    "#SystemMessage → instruções do sistema (prompt do sistema).\n",
    "#HumanMessage → mensagens enviadas pelo usuário.\n",
    "\n",
    "from langchain_core.messages import SystemMessage, HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9344adb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dict = dicionário (chave: valor)\n",
    "#List = lista (coleção de itens)\n",
    "#Literal = literal (valor fixo, permitido apenas alguns)\n",
    "\n",
    "from typing import Dict, List, Literal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f0eda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria um 'chain' estruturado que força o LLM a produzir saídas no formato TriagemOut\n",
    "triagem_chain = llm_triagem.with_structured_output(TriagemOut)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2e144d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a função de triagem que processa a mensagem do usuário\n",
    "# Envia a mensagem e o prompt do sistema para o LLM e valida a resposta com TriagemOut\n",
    "\n",
    "def triagem(mensagem: str) -> Dict:\n",
    "    saida: TriagemOut = triagem_chain.invoke([\n",
    "        SystemMessage(content=TRIAGEM_PROMPT),\n",
    "        HumanMessage(content=mensagem)\n",
    "    ])\n",
    "\n",
    "    return saida.model_dump()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d7e8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testes da função de triagem\n",
    "testes = [\"Posso reembolsar a internet?\",\n",
    "          \"Quero mais 5 dias de trabalho remoto. Como faço?\",\n",
    "          \"Posso reembolsar cursos ou treinamentos da Alura?\",\n",
    "          \"Quantas capivaras tem no Rio Pinheiros?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e9a8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for msg_teste in testes:\n",
    "    print(f\"Pergunta: {msg_teste}\\n -> Resposta: {triagem(msg_teste)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698e5dfa",
   "metadata": {},
   "source": [
    "3. Processamento de Documentos (RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c756b801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa bibliotecas para manipulação de caminhos de arquivo e carregamento de PDFs\n",
    "from pathlib import Path\n",
    "from langchain_community.document_loaders import PyMuPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd0a709",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cria uma lista vazia que irá guardar todos os documentos extraídos do PDF\n",
    "docs = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc0b96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percorre todos os arquivos PDF na pasta './pdfs/'\n",
    "# Carrega o texto de cada PDF usando PyMuPDFLoader e adiciona à lista 'd\n",
    "for n in Path(\"./pdfs/\").glob(\"*.pdf\"):\n",
    "    try:\n",
    "        loader = PyMuPDFLoader(str(n))\n",
    "        docs.extend(loader.load())\n",
    "        print(f\"Carregado com sucesso arquivo {n.name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao carregar arquivo {n.name}: {e}\")\n",
    "\n",
    "print(f\"Total de documentos carregados: {len(docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc81e793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa o separador de texto recursivo\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "# Inicializa o separador de texto para dividir documentos em 'chunks'\n",
    "# chunk_size: Tamanho máximo de cada pedaço de texto (ex: 300 caracteres)\n",
    "# chunk_overlap: Quantidade de caracteres que os pedaços se sobrepõem (ex 30 caracteres)\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=30)\n",
    "chunks = splitter.split_documents(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce9c2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Percorre todos os pedaços de texto dos PDFs e imprime cada um, separado por linhas para facilitar a leitura.\n",
    "for chunk in chunks:\n",
    "    print(chunk)\n",
    "    print(\"------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c054ee92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#page_content é o texto “de verdade” que você quer que seu agente de IA leia e processe\n",
    "for chunk in chunks:\n",
    "    print(chunk.page_content)\n",
    "    print(\"------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d31af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa a classe para gerar embeddings do Google Gemini\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2604809c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializa o gerador de embeddings com o modelo Gemini\n",
    "# Embeddings: Representações numéricas de texto que permitem comparar e buscar textos semanticamente\n",
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/gemini-embedding-001\",\n",
    "    google_api_key=GOOGLE_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e9bbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa a biblioteca FAISS para armazenamento e busca rápida de vetores\n",
    "from langchain_community.vectorstores import FAISS\n",
    "# Cria um Vectorstore FAISS a partir dos chunks e embeddings\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "# Cria um retriever para buscar documentos relevantes com base na similaridade semântica\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity_score_threshold\",\n",
    "                                     search_kwargs={\"score_threshold\":0.3, \"k\": 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab0f899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa classes para criar prompts de chat e combinar documentos\n",
    "from langchain_core.prompts import ChatPromptTemplate #permite criar prompts personalizados para chats\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain #combina vários chunks/documentos em uma resposta única usando um LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab984b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define o prompt para o RAG (Retrieval-Augmented Generation)\n",
    "# System: Instruções para o modelo (assistente de políticas, responder apenas com contexto)\n",
    "# Human: Pergunta do usuário e contexto recuperado\n",
    "prompt_rag = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"Você é um Assistente de Políticas Internas (RH/IT) da empresa Carraro Desenvolvimento. \"\n",
    "     \"Responda SOMENTE com base no contexto fornecido. \"\n",
    "     \"Se não houver base suficiente, responda apenas 'Não sei'.\"),\n",
    "\n",
    "    (\"human\", \"Pergunta: {input}\\n\\nContexto:\\n{context}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34668c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria um 'document_chain' que conecta o LLM ao prompt RAG para processar documentos\n",
    "document_chain = create_stuff_documents_chain(llm_triagem, prompt_rag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b77414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa bibliotecas para expressões regulares e manipulação de caminhos\n",
    "import re, pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfd69f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _clean_text(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", s or \"\").strip()\n",
    "# Funções para formatar as citações dos documentos relevantes\n",
    "\n",
    "def extrair_trecho(texto: str, query: str, janela: int = 240) -> str:\n",
    "    txt = _clean_text(texto)\n",
    "    termos = [t.lower() for t in re.findall(r\"\\w+\", query or \"\") if len(t) >= 4]\n",
    "    pos = -1\n",
    "    for t in termos:\n",
    "        pos = txt.lower().find(t)\n",
    "        if pos != -1: break\n",
    "    if pos == -1: pos = 0\n",
    "    ini, fim = max(0, pos - janela//2), min(len(txt), pos + janela//2)\n",
    "    return txt[ini:fim]\n",
    "\n",
    "def formatar_citacoes(docs_rel: List, query: str) -> List[Dict]:\n",
    "    cites, seen = [], set()\n",
    "    for d in docs_rel:\n",
    "        src = pathlib.Path(d.metadata.get(\"source\",\"\")).name\n",
    "        page = int(d.metadata.get(\"page\", 0)) + 1\n",
    "        key = (src, page)\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        cites.append({\"documento\": src, \"pagina\": page, \"trecho\": extrair_trecho(d.page_content, query)})\n",
    "    return cites[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a81cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função principal para perguntar à política usando RAG\n",
    "# Implementa lógica de segurança: só responde se houver contexto válido\n",
    "def perguntar_politica_RAG(pergunta: str) -> Dict:\n",
    "    docs_relacionados = retriever.invoke(pergunta)\n",
    "\n",
    "    if not docs_relacionados:\n",
    "        return {\"answer\": \"Não sei.\",\n",
    "                \"citacoes\": [],\n",
    "                \"contexto_encontrado\": False}\n",
    "\n",
    "    answer = document_chain.invoke({\"input\": pergunta,\n",
    "                                    \"context\": docs_relacionados})\n",
    "\n",
    "    txt = (answer or \"\").strip()\n",
    "\n",
    "    if txt.rstrip(\".!?\") == \"Não sei\":\n",
    "        return {\"answer\": \"Não sei.\",\n",
    "                \"citacoes\": [],\n",
    "                \"contexto_encontrado\": False}\n",
    "\n",
    "    return {\"answer\": txt,\n",
    "            \"citacoes\": formatar_citacoes(docs_relacionados, pergunta),\n",
    "            \"contexto_encontrado\": True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b3018d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testes da função RAG\n",
    "testes = [\"Posso reembolsar a internet?\",\n",
    "          \"Quero mais 5 dias de trabalho remoto. Como faço?\",\n",
    "          \"Posso reembolsar cursos ou treinamentos da Alura?\",\n",
    "          \"Quantas capivaras tem no Rio Pinheiros?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da55a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for msg_teste in testes:\n",
    "    resposta = perguntar_politica_RAG(msg_teste)\n",
    "    print(f\"PERGUNTA: {msg_teste}\")\n",
    "    print(f\"RESPOSTA: {resposta['answer']}\")\n",
    "    if resposta['contexto_encontrado']:\n",
    "        print(\"CITAÇÕES:\")\n",
    "        for c in resposta['citacoes']:\n",
    "            print(f\" - Documento: {c['documento']}, Página: {c['pagina']}\")\n",
    "            print(f\"   Trecho: {c['trecho']}\")\n",
    "        print(\"------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463086cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa TypedDict e Optional para definir o estado do agente\n",
    "from typing import TypedDict, Optional\n",
    "# Define a estrutura do estado do agente usando TypedDict\n",
    "# Armazena a pergunta, resultados da triagem, resposta, citações, sucesso do RAG e ação final\n",
    "class AgentState(TypedDict, total = False):\n",
    "    pergunta: str\n",
    "    triagem: dict\n",
    "    resposta: Optional[str]\n",
    "    citacoes: List[dict]\n",
    "    rag_sucesso: bool\n",
    "    acao_final: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a64504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nó de triagem: executa a função de triagem e atualiza o estado\n",
    "def node_triagem(state: AgentState) -> AgentState:\n",
    "    print(\"Executando nó de triagem...\")\n",
    "    return {\"triagem\": triagem(state[\"pergunta\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf48e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nó de auto-resolver: tenta resolver a pergunta usando RAG e atualiza o estado\n",
    "def node_auto_resolver(state: AgentState) -> AgentState:\n",
    "    print(\"Executando nó de auto_resolver...\")\n",
    "    resposta_rag = perguntar_politica_RAG(state[\"pergunta\"])\n",
    "\n",
    "    update: AgentState = {\n",
    "        \"resposta\": resposta_rag[\"answer\"],\n",
    "        \"citacoes\": resposta_rag.get(\"citacoes\", []),\n",
    "        \"rag_sucesso\": resposta_rag[\"contexto_encontrado\"],\n",
    "    }\n",
    "\n",
    "    if resposta_rag[\"contexto_encontrado\"]:\n",
    "        update[\"acao_final\"] = \"AUTO_RESOLVER\"\n",
    "\n",
    "    return update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b67cc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nó de pedir informação: solicita mais detalhes ao usuário e atualiza o estado\n",
    "\n",
    "def node_pedir_info(state: AgentState) -> AgentState:\n",
    "    print(\"Executando nó de pedir_info...\")\n",
    "    faltantes = state[\"triagem\"].get(\"campos_faltantes\", [])\n",
    "    if faltantes:\n",
    "        detalhe = \",\".join(faltantes)\n",
    "    else:\n",
    "        detalhe = \"Tema e contexto específico\"\n",
    "\n",
    "    return {\n",
    "        \"resposta\": f\"Para avançar, preciso que detalhe: {detalhe}\",\n",
    "        \"citacoes\": [],\n",
    "        \"acao_final\": \"PEDIR_INFO\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f51af29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nó de abrir chamado: simula a abertura de um chamado e atualiza o estado\n",
    "def node_abrir_chamado(state: AgentState) -> AgentState:\n",
    "    print(\"Executando nó de abrir_chamado...\")\n",
    "    triagem = state[\"triagem\"]\n",
    "\n",
    "    return {\n",
    "        \"resposta\": f\"Abrindo chamado com urgência {triagem['urgencia']}. Descrição: {state['pergunta'][:140]}\",\n",
    "        \"citacoes\": [],\n",
    "        \"acao_final\": \"ABRIR_CHAMADO\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79046b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Palavras-chave para decidir se um chamado deve ser aberto\n",
    "KEYWORDS_ABRIR_TICKET = [\"aprovação\", \"exceção\", \"liberação\", \"abrir ticket\", \"abrir chamado\", \"acesso especial\"]\n",
    "\n",
    "# Função de roteamento: decide o próximo nó após a triagem\n",
    "def decidir_pos_triagem(state: AgentState) -> str:\n",
    "    print(\"Decidindo após a triagem...\")\n",
    "    decisao = state[\"triagem\"][\"decisao\"]\n",
    "\n",
    "    if decisao == \"AUTO_RESOLVER\": return \"auto\"\n",
    "    if decisao == \"PEDIR_INFO\": return \"info\"\n",
    "    if decisao == \"ABRIR_CHAMADO\": return \"chamado\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7694d4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função de roteamento: decide o próximo nó após a tentativa de autoresolução\n",
    "def decidir_pos_auto_resolver(state: AgentState) -> str:\n",
    "    print(\"Decidindo após o auto_resolver...\")\n",
    "\n",
    "    if state.get(\"rag_sucesso\"):\n",
    "        print(\"Rag com sucesso, finalizando o fluxo.\")\n",
    "        return \"ok\"\n",
    "\n",
    "    state_da_pergunta = (state[\"pergunta\"] or \"\").lower()\n",
    "\n",
    "    if any(k in state_da_pergunta for k in KEYWORDS_ABRIR_TICKET):\n",
    "        print(\"Rag falhou, mas foram encontradas keywords de abertura de ticket. Abrindo...\")\n",
    "        return \"chamado\"\n",
    "\n",
    "    print(\"Rag falhou, sem keywords, vou pedir mais informações...\")\n",
    "    return \"info\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c655626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa classes para construir o grafo de estados\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "# Inicializa o workflow como um grafo de estados\n",
    "workflow = StateGraph(AgentState)\n",
    "# Adiciona os nós ao grafo\n",
    "workflow.add_node(\"triagem\", node_triagem)\n",
    "workflow.add_node(\"auto_resolver\", node_auto_resolver)\n",
    "workflow.add_node(\"pedir_info\", node_pedir_info)\n",
    "workflow.add_node(\"abrir_chamado\", node_abrir_chamado)\n",
    "# Define as arestas (transições) do grafo\n",
    "workflow.add_edge(START, \"triagem\")\n",
    "workflow.add_conditional_edges(\"triagem\", decidir_pos_triagem, {\n",
    "    \"auto\": \"auto_resolver\",\n",
    "    \"info\": \"pedir_info\",\n",
    "    \"chamado\": \"abrir_chamado\"\n",
    "})\n",
    "\n",
    "workflow.add_conditional_edges(\"auto_resolver\", decidir_pos_auto_resolver, {\n",
    "    \"info\": \"pedir_info\",\n",
    "    \"chamado\": \"abrir_chamado\",\n",
    "    \"ok\": END\n",
    "})\n",
    "\n",
    "workflow.add_edge(\"pedir_info\", END)\n",
    "workflow.add_edge(\"abrir_chamado\", END)\n",
    "# Compila o grafo para execução\n",
    "grafo = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acb03c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa bibliotecas para exibir o grafo\n",
    "from IPython.display import display, Image\n",
    "# Gera e exibe a imagem do grafo (requer instalação de 'graphviz')\n",
    "graph_bytes = grafo.get_graph().draw_mermaid_png()\n",
    "display(Image(graph_bytes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2a51a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testes do fluxo completo do agente\n",
    "\n",
    "testes = [\"Posso reembolsar a internet?\",\n",
    "          \"Quero mais 5 dias de trabalho remoto. Como faço?\",\n",
    "          \"Posso reembolsar cursos ou treinamentos da Alura?\",\n",
    "          \"É possível reembolsar certificações do Google Cloud?\",\n",
    "          \"Posso obter o Google Gemini de graça?\",\n",
    "          \"Qual é a palavra-chave da aula de hoje?\",\n",
    "          \"Quantas capivaras tem no Rio Pinheiros?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1d39f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "for msg_test in testes:\n",
    "    resposta_final = grafo.invoke({\"pergunta\": msg_test})\n",
    "\n",
    "    triag = resposta_final.get(\"triagem\", {})\n",
    "    print(f\"PERGUNTA: {msg_test}\")\n",
    "    print(f\"DECISÃO: {triag.get('decisao')} | URGÊNCIA: {triag.get('urgencia')} | AÇÃO FINAL: {resposta_final.get('acao_final')}\")\n",
    "    print(f\"RESPOSTA: {resposta_final.get('resposta')}\")\n",
    "    if resposta_final.get(\"citacoes\"):\n",
    "        print(\"CITAÇÕES:\")\n",
    "        for citacao in resposta_final.get(\"citacoes\"):\n",
    "            print(f\" - Documento: {citacao['documento']}, Página: {citacao['pagina']}\")\n",
    "            print(f\"   Trecho: {citacao['trecho']}\")\n",
    "    print(\"------------------------------------\")\n",
    "    # Controla a taxa de requisições para evitar limites da API\n",
    " \n",
    "    time.sleep(8)  # ajuste se ainda der 429\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
